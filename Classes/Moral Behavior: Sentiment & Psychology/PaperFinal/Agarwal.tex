\documentclass[11pt, oneside]{article}
\usepackage{geometry}
\geometry{letterpaper}
\usepackage{graphicx}
\usepackage{amssymb}
\linespread{2.25}	

\title{Exploring moral behavior of thinking machines}
\author{Abhi Agarwal}
\date{}
\begin{document}
\maketitle

\par Did morality evolve? Frans de Waal centralizes this idea and furthers it by asking how morality evolved. Opposing de Waal on the idea are a small group of biologists. They argue that morality is a construct or an idea that is unique to human beings, and the purpose of morality is to minimize our animalistic instincts. De Waal gives accounts for the idea that morality has evolved continuously and has evolved from Chimpanzees, Bonobos, and Great Apes. He examines different species of animals and points out that primates close to us in the ladder of evolution display a large number of moral traits. Traits such as peacemaking, empathy, sympathy, and many more. For example, if two male chimpanzees are in a quarrel, the female chimpanzees will often mediate and help them settle their quarrel in order to restore the balance in the community. 

\par The smaller group of biologists consists of credible individuals such as Thomas Huxley and George Williams. De Waal, in his book, coined the term Veneer theory to encapsulate their arguments. Veneer theory ``assumes that deep down we are not truly moral. It views morality as a cultural overlay, a thin veneer hiding an otherwise selfish and brutish nature" (De Waal, 6). The Veneer theory disregards any connection between morality of humans and the tendencies animals have, and so implies that animals and humans are distinct in this regard. De Waal criticizes this theory in his book primarily for two purposes. The first, the theory lacks proof - de Waal points out that there is no empirical evidence to back the theory. The second, to de Waal it seems unlikely that morality is improved by choice and not through genes. I, personally, side with de Waal. I agree that morality is a product of social evolution - I believe that morality is continuously evolving. Through the proposition of this criticism, de Waal implies that the building blocks of moral agency are apparent within primates (such as Bonobos).

\par Why is it necessary for some moral behavior to have existed in animals? There is not much of a need of moral behavior when an animal is living alone. However, when animals live within packs or groups then the need for restraint on behavior becomes necessary. One of the truths that is taught to us is that we live in groups because the chance of survival and reproduction are much higher than when living alone. In order for animals to be able to adapt in groups they need to change or mask the primal animalistic nature that they rely on for survival when they live alone. De Waal shows this idea by providing examples of animals that live in groups. In addition, Edward Wilson furthers this idea in his book `Journey to the Ants'. Wilson describes ant colonies and their successes as a group. Notably, to assist the queen, female ants give up on reproducing to help raise their brothers and sisters. The female ants reduce the competition there is for mating and foster a sense of cooperation within the colony. These traits and common goals improves the colony's chance to exist for decades. In this regard, ants display the trait of altruism. 

\par De Waal points out that there are aspects of morality that are unique to human beings. He notes that the ability to weigh, reason, and judge two separate moral decisions and choose an outcome is one of them. The other is the ability to be impartial and spectate a situation. In addition, there are also fundamental differences in our society and biologically. We have evolved to a point where we don't necessarily form social groups in order for survival, but for cultural interests, religious interests, etc. We also differentiate in the ability to communicate through an established language. This is an important aspect that helps us in making our moral decisions. The biggest differentiator between human beings and animals, to me, is the idea that we're able to express things we're thinking about in a formal and clear manner. We're able to have a discourse about the disagreements we have, and debate whether those disagreements are valid or not. These aspects are critical in understanding the evolution of our morality.

\par De Waal manages to establish and solidify the idea that there was an evolution of morality, and the roots of morality can be seen in primates. To me, thinking about the evolution of morality is intriguing. We're attempting to build machines that can think and potentially machines that will walk among us. Soon we will progress into developing artificially intelligent machines. This raises a question, can morality evolve to being implemented digitally? How could morality evolve to thinking machines as it did from primates to us?

\par There exists a thought problem in the field of Artificial Intelligence that makes this question worth exploring. Hypothetically, I order an AI to build paper clips. The AI would obey its creator and would begin creating paper clips. Eventually it would turn the entire universe into paper clips. The reasoning behind this is that the AI doesn't have an understanding of limitations like we do. An AI only knows its task and that it has to keep working on that task until it reaches a goal. I believe that our understanding of limitations stands from our morality, and our knowledge of what is right and wrong. How can we extend this moral behavior into thinking machines?

\par What are thinking machines? What exactly are machines that have the ability to make decisions, and mirror our intelligence? In the Aristotelian framework `virtue' is moral responsibility, and represents the bestowment of praise or blame. To qualify for this bestowment of praise or blame must be done voluntarily by the agent. In Nicomachean Ethics, Aristotle writes ``virtue is about feelings and actions. These receive praise or blame if they are voluntary, but pardon, sometimes even pity, if they are involuntary" (Aristotle, 30). The first condition for our thinking machine must be that it is able to perform actions voluntarily. Through this condition the thinking machine becomes an agent that is morally praiseworthy. The moral actions of a thinking machine should be indistinguishable from any moral person. The second condition is that thinking machines are able to make intelligent decisions. They use previous knowledge and experiences to make decisions, and weigh the outcome of their actions. The last condition is that thinking machines are able to adapt and learn from their experiences - meaning that their `system' can be adapted by learning new information. The big assumption here is that there is a motivation for a thinking machine to be moral. 

\par Firstly, what does it mean for morality to evolve beyond human beings? Morality evolved from primates to human beings because there were characteristics that made us unique and different to primates. There exists a similar case between us and thinking machines. In addition, thinking machines are much more different biologically to us than we are different to primates. The word evolution, in this context, doesn't necessarily mean biological evolution. When we design these thinking machines we've to think about what approaches we will take in designing its consciousness and understanding of morality. In this way, we are evolving our morality. Morality is evolving to suit its new surroundings, and we're adapting our morality depending on the unique nature of these machines. For example, machines can't feel pain, and in designing their morality we have to consider this fact. Thinking machines must acknowledge that they aren't able to cause others pain.

\par Furthermore, we have established that morality is evolved, and that we're exploring the idea of the evolution of morality beyond us. In order to extend our understanding, we have to understand how we become the moral agents that we are. There are certain traits that are inherent to us, but there are also morals that we learn from our surroundings. Things like taking care of your offspring is hard-wired into us, but is the principle that you can't steal from someone else? In her book `Braintrust', one of the points in Churchland's thesis is the idea that moral codes are, like language, culture specific, and are learnt as we grow up. She also points out that we have a motivation to learn to be moral. The average man learns and stays moral primarily for success in the future where success could be: monetary success, social success, religious, etc. Here, we making another assumption that the future aims of the thinking machines are like ours. We have designed these future machines to have aims and motivations like ours of the future.

\par In addition, there is a big discussion in the field of AI regarding how each thinking machine should be `born'. There are two distinct routes. The first being that thinking machines should be `born' as children, and then learn and adapt the same way as human children do. The second being that thinking machines should be `born' as adults, and have programmed some knowledge about their purpose. Both are extremely flexible, and have their own series of pros and cons. The purpose of being born as an adult is to quickly be able to utilize the skill that the thinking machine is created for. If the thinking machine is created to clean the house, then we don't necessarily want to have to wait 21 years before it would be able to do so. However, moral rules would have to be programmed within the adults in order for them to have an understanding of right and wrong. Here, we also further our definition of thinking machines as we establish the idea that their decision-making must be stochastic. 

\par If thinking machines were to be born as children, in the same way we are, then they would be able to mirror the development of a human child. They would be able to go through the same process of understanding what not to do, and what to do. 

\par Arguments for a goodness within us. 

\par Morality and its connection to emotion.

\par What goals would we program a computer to fulfill? Churchland suggests that morals are not objectives or transcendent, but they feel as though they are. In order for us to allow computers to follow these morals or program them, we have to make them objectives or checks that occur. 

% http://philosophicaldisquisitions.blogspot.com/2014/08/bostrom-on-superintelligence-6.html
\par Since we could be able to program a computer to make certain deliberations, what should the deliberations be? Should we even make deliberations? There are four possible ways that researchers have thought to be valid options. They are: Direct Specification, Domesticity, Indirect Normativity, and Augmentation.

\par Firstly, Direct Specification is directly embedding the right set of motivations into the programming for the thinking machines. The difficulty here is ``in determining which rules or values we would wish the AI to be guided by and the difficulties in expressing those rules or values in computer-readable code" (Bostrom, 139). This was made popular by Isaac Asimov through his Three Laws of Robotics. The three being: ``A robot may not injure a human being or, through inaction, allow a human being to come to harm", ``A robot must obey the orders given it by human beings, except where such orders would conflict with the First Law", ``A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws" (Asimov, 27). The trouble here is that you can only define them once. They are also open to interpretation - Bostrom points out that the most optimal choice by the AI would be to put the humans into artificially induced comas thereby keeping them safe. For this to execute well we would have to think about every case and edge-case, and define each thing carefully. In addition, given this option it's difficult to think about how the thinking machine would do in the counterexample to consequentialism: the trolley problem. Bostrom writes ``A small error in either the philosophical account or its translation into code could have catastrophic consequences" (Bostrom, 140). 

\par Secondly, Domesticity is 

\par Thirdly, the basic idea behind Indirect Normativity is ``rather than specifying a concrete normative standard directly, we specify a process for deriving a standard" (Bostrom, 141). This follows the ideal observer theory in ethics, and we would guide the thinking machine to mirror an ideal and hyper-rational human. The only downside here is that we give the thinking machine a lot of leeway, and we are uncertain of the outcome that could produce. 

\par Lastly, Augmentation is 

\par Which of the options are optimal? 

\par Frameworks for morality in the future. Perfect knowledge of all actions. 

\par In conclusion, (We aren't able to predict how society will evolve for thinking machines or if that will happen).

\begin{thebibliography}{9}
\bibitem{Superintelligence}
  Bostrom, Nick. 
  \emph{Superintelligence: The Coming Machine Intelligence Revolution}.
  Oxford: Oxford UP, 2013. 
  Print.
\bibitem{Robot}
  Asimov, Isaac.
  \emph{I, Robot}.
  New York, NY: Bantam, Spectra, 1950.
  Print.
\bibitem{Primates} 
  De Waal, F. B. M., Stephen Macedo, Josiah Ober, and Robert Wright.
  \emph{Primates and Philosophers: How Morality Evolved}.
  Princeton, NJ: Princeton UP, 2006.
  Print.
\bibitem{Nicomachean} 
  Aristotle, and Terence Irwin.
  \emph{Nicomachean Ethics}.
  Indianapolis, IN: Hackett Pub., 1999.
  Print.
\bibitem{Braintrust}
  Churchland, Patricia Smith.
  \emph{Braintrust: What Neuroscience Tells Us about Morality}.
  Princeton, NJ: Princeton UP, 2011. 
  Print.
\end{thebibliography}
\end{document}  